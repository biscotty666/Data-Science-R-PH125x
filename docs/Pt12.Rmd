---
title: "12 Summary Statistics"
output:
  html_document:
    df_print: paged
    css: "style.css"
    toc: true
---

[Book](http://rafalab.dfci.harvard.edu/dsbook/r-basics.html)

R commands in this chapter

|[`coord_cartesian`](#coord_cartesian)|
[`IQR`](#iqr)|
[`mad`](#mad)|
[`pnorm`](#pnorm)|
[`qnorm`](#qnorm)|
[`scale`](#scale)|

# 12.1 Variable types

- __numeric__
  - discrete
  - continuous
- __categorical__
  - ordinal
  - not

Some categorical data can be ordered even if they are not numbers, such as spiciness (mild, medium, hot). In statistics textbooks, ordered categorical data are referred to as *ordinal* data.

Keep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, <span color="yellow">for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable.</span> But, indeed, there are examples that can be considered both numerical and ordinal. 

# 12.2 Distributions

The most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for readers of this book. For example, with categorical data, the distribution simply describes the proportion of each unique category. Here is an example with US state regions:

```{r}
prop.table(table(state.region))
```

When the data is numerical, the task of constructing a summary based on the distribution is more challenging. We introduce an artificial, yet illustrative, motivating problem that will help us introduce the concepts needed to understand distributions.

## 12.2.1 Case study: describing student heights

Pretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the heights data frame:

```{r}
library(tidyverse)
library(dslabs)
data("heights")
```

One way to convey the heights to ET is to simply send him this list of 1050 heights. But there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights. We examine the female height data in Section 12.7.1.

It turns out that, in some cases, the average and the standard deviation are pretty much all we need to understand the data. We will learn data visualization techniques that will help us determine when this two number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.

## 12.2.2 Empirical cumulative distribution functions

Numerical data that are not categorical also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of `68.503937007874` inches and only one student reported a height `68.8976377952756` inches. We assume that they converted from 174 and 175 centimeters, respectively.

Statistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data entries $x$ that are below $a$, for all possible values of $a$. This function is called the <span class="green">empirical cumulative distribution function (eCDF)</span> and often denoted with $F$:

$$F(a)=\text{Proportion of data points >=a}$$
Here is a plot of $F$ for the male height data:

```{r}
heights |>
  filter(sex == "Male") |>
  ggplot(aes(height)) +
  stat_ecdf() +
  ylab("F(a)") + xlab("a")
```

<span class="green">Similar to what the frequency table does for categorical data, the eCDF defines the distribution for numerical data.</span> From the plot, we can see that 16% of the values are below 65, since $F(66)=0.164$, or that 84% of the values are below 72, since $F(72)=0.841$, and so on. In fact, we can report the proportion of values between any two heights, say $a$ and $b$, by computing $F(b)−F(a)$. This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousand words”, in this case, a picture is as informative as 812 numbers.

Note: the reason we add the word empirical is because, as we will see in 13.10.1, the cumulative distribution function (CDF) can be defined mathematically, meaning without any data.

## 12.2.3 Histograms

Although the eCDF concept is widely discussed in statistics textbooks, the summary plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce summaries that are much easier to interpret. 

The simplest way to make a histogram is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: $(49.5, 50.5],(50.5, 51.5],(51.5,52.5],(52.5,53.5],...,(82.5,83.5]$
```{r}
heights |>
  filter(sex == "Male") |>
  ggplot(aes(height)) +
  geom_histogram(binwidth = 1, 
                 color = "black")
```
As you can see in the figure above, <span class="green">a histogram is similar to a barplot, but it differs in that the x-axis is numerical, not categorical.</span>

If we send this plot to ET, he will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, ET could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of `r sum(heights$sex=="Male")` heights with about 30 bin counts.

What information do we lose?  Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers.

## 12.2.4 Smoothed density

Smooth density plots are similar to histograms, but the data is not divided into bins. Here is what a smooth density plot looks like for our heights data:
```{r}
heights |>
  filter(sex == "Male") |>
  ggplot(aes(height)) +
  geom_density(fill = "darkgoldenrod", 
               alpha = .2)
```
In this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to _density_.

To understand the smooth densities, we have to understand _estimates_, a topic we don't cover until later. However, we provide a heuristic explanation to help you understand the basics.

The main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of `r sum(heights$sex=="Male")` male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let's say there are 1,000,000 of these measurements. This list of values has a distribution, like any list of values, and this larger distribution is really what we want to report to ET since it is much more general. Unfortunately, we don't get to see it. 

However, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don't have big jumps in the heights of consecutive bins. Below we have a hypothetical histogram with bins of size 1:
```{r}
set.seed(1988)
x <- data.frame(height = c(rnorm(1000000, 69, 3),
                           rnorm(1000000, 65, 3)))
x |> ggplot(aes(height)) + geom_histogram(binwidth = 1, color = "black")
```
The smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5, and 0.1:

```{r}
xplt <- x |> ggplot(aes(height))
p1 <- xplt + geom_histogram(binwidth = 1) + ggtitle("binwidth = 1")
p2 <- xplt + geom_histogram(binwidth = 0.5) + ggtitle("binwidth = 0.5")
p3 <- xplt + geom_histogram(binwidth = 0.1) + ggtitle("binwidth = 0.1")
library(gridExtra)
grid.arrange(p1, p2, p3, nrow = 1)
```

The smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts:
```{r}
xplt +
  geom_histogram(aes(y = ..density..), binwidth = 0.1) +
  geom_line(stat = "density")
```

Now, back to reality. We don’t have millions of measurements. Instead, we have 812 and we can’t make a histogram with very small bins.

We therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars. The following plots demonstrate the steps that lead to a smooth density:

```{r}
hist1 <- 
  heights |>
  filter(sex == "Male") |>
  ggplot(aes(height)) +
  geom_histogram(aes(y = ..density..),
                 binwidth = 1,
                 color = "black")
hist2 <- 
  hist1 +
  geom_line(stat = "density")
hist3 <- 
  hist1 +
  geom_point(data = ggplot_build(hist2)$data[[1]],
             aes(x,y),
             col = "blue")
hist4 <- 
  ggplot() +
  geom_point(data = ggplot_build(hist2)$data[[1]],
             aes(x,y),
             col = "blue") +
  xlab("height") + ylab("density")
hist5 <- 
  hist4 +
  geom_line(data = ggplot_build(hist2)$data[[2]], aes(x,y))
hist6 <- 
  heights |>
  filter(sex == "Male") |>
  ggplot(aes(height)) +
  geom_density(alpha = 0.2,
               fill = "darkgoldenrod",
               col = 0) +
  geom_line(stat = "density") +
  scale_y_continuous(limits = layer_scales(hist2)$y$range$range)

grid.arrange(hist1, hist2, hist3, hist4, hist5, hist6, nrow=2)
```

However, remember that _smooth_ is a relative term. We can actually control the _smoothness_ of the curve that defines the smooth density through an option in the function that computes the smooth density curve. Here are two examples using different degrees of smoothness on the same histogram:

```{r}
tmp <- 
  heights |> 
  ggplot(aes(height)) +
  geom_histogram(aes(y = ..density..),
                 binwidth = 1,
                 alpha = 0.5)
p1 <- tmp +
  geom_line(stat = "density", adjust = 0.5)
p2 <- tmp +
  geom_line(stat = "density", adjust = 2)

grid.arrange(p1, p2, ncol=2)
```

We need to make this choice with care as the resulting summary can change our interpretation of the data. We should select a degree of smoothness that we can defend as being representative of the underlying data. In the case of height, we really do have reason to believe that the proportion of people with similar heights should be the same. For example, the proportion that is 72 inches should be more similar to the proportion that is 71 than to the proportion that is 78 or 65. This implies that the curve should be pretty smooth; that is, the curve should look more like the example on the right than on the left.

While the histogram is an assumption-free summary, the smoothed density is based on some assumptions.

Note that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:
```{r}
d <- with(heights, density(height[sex == "Male"]))
tmp <- data.frame(height = d$x, density = d$y)
tmp |>
  ggplot(aes(height, density)) +
  geom_line() +
  geom_area(aes(x = height, density),
            data = filter(tmp, between(height, 65, 68)),
            alpha = 0.4,
            fill = "darkgoldenrod")
```

The proportion of this area is about 0.3, meaning that about 30% of male heights are between 65 and 68 inches.

By understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:
```{r}
heights |>
  filter(sex == "Male") |>
  ggplot(aes(height)) +
  geom_density(alpha = 0.4,
            fill = "darkgoldenrod")
```

# 12.3 Exercises

1. In the `murders` dataset, the region is a categorical variable and the following is its distribution:

```{r}
data("murders")
ds_theme_set()
str(murders)
```

```{r}
murders |>
  group_by(region) |>
  summarise(n = n()) |>
  mutate(Proportion = n / sum(n),
         region = reorder(region, Proportion)) |>
  ggplot(aes(region, Proportion, fill = region)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  xlab("")
```
To the closest 5%, what proportion of the states are in the North Central region? <span class="green">25%</span>

2. Which of the following is true:

a. The graph above is a histogram.
b. The graph above shows only four numbers with a bar plot.
c. <span class="green">Categories are not numbers, so it does not make sense to graph the distribution.</span>
d. The colors, not the height of the bars, describe the distribution.

3. The plot below shows the eCDF for male heights:
```{r}
heights |>
  filter(sex == "Male") |>
  ggplot(aes(height)) +
  stat_ecdf() +
  ylab("F(a)") + xlab("a")
```

Based on the plot, what percentage of males are shorter than 75 inches?

a. 100%
b. <span class="green">95%</span>
c. 80%
d. 72 inches

4. To the closest inch, what height `m` has the property that 1/2 of the male students are taller than `m` and 1/2 are shorter?

a. 61 inches
b. 64 inches
c. <span class="green">69 inches</span>
d. 74 inches

5. Here is an eCDF of the murder rates across states:

```{r}
str(murders)
```
```{r}
murders |>  
  mutate(murder_rate = total / population * 10^5) |>
  ggplot(aes(murder_rate)) +
  stat_ecdf()
```

Knowing that there are 51 states (counting DC) and based on this plot, how many states have murder rates larger than 10 per 100,000 people?

a. <span class="green">1</span>
b. 5
c. 10
d. 50

6. Based on the eCDF above, which of the following statements are true:

a. About half the states have murder rates above 7 per 100,000 and the other half below.
b. Most states have murder rates below 2 per 100,000.
c. All the states have murder rates above 2 per 100,000.
d. <span class="green">With the exception of 4 states, the murder rates are below 5 per 100,000.</span>

7. Below is a histogram of male heights in our heights dataset:
```{r}
heights |>
  filter(sex == "Male") |>
  ggplot(aes(height)) +
  geom_histogram(binwidth = 1, color = "black")
```

Based on this plot, how many males are between 63.5 and 65.5?

a. 10
b. 24
c. <span class="green">34</span>
d. 100

8. About what percentage are shorter than 60 inches?

a. <span class="green">1%</span>
b. 10%
c. 25%
d. 50%

9. Based on the density plot below, about what proportion of US states have populations larger than 10 million?
```{r}
murders |>
  ggplot(aes(population/10^6)) +
  geom_density(fill = "darkgoldenrod") +
  scale_x_log10()
```

a. 0.02
b. <span class="green>0.15</span>
c. 0.50
d. 0.55

10. Below are three density plots. Is it possible that they are from the same dataset?
```{r}
murders |>
  ggplot(aes(population / 10^6)) +
  xlab("Population in millions") ->
  tmp
p1 <- 
  tmp +
  geom_density(bw = 5, fill = "grey") +
  ggtitle("1")
p2 <- 
  tmp +
  geom_density(bw = .05, fill = "grey") +
  scale_x_log10() +
  ggtitle("2")
p3 <- 
  tmp +
  geom_density(bw = 1, fill = "grey") +
  scale_x_log10() +
  ggtitle("3")
grid.arrange(p1, p2, p3, ncol = 2)
```

Which of the following statements is true:

a. It is impossible that they are from the same dataset.
b. They are from the same dataset, but the plots are different due to code errors.
c. They are the same dataset, but the first and second plot undersmooth and the third oversmooths.
d. <span class="green">They are the same dataset, but the first is not in the log scale, the second undersmooths, and the third oversmooths.</span>

# 12.4 The normal distribution

The normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these later. Here we focus on how the normal distribution helps us summarize data.

Rather than using data, the normal distribution is defined with a mathematical formula. For any interval $(a,b)$, the proportion of values in that interval can be computed using this formula:

$$\mbox{Pr}(a < x \leq b) = \int_a^b \frac{1}{\sqrt{2\pi}s} e^{-\frac{1}{2}\left( \frac{x-m}{s} \right)^2} \, dx$$

where $m$ is the mean and $s$ is the standard deviation.

<span class="green">The distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average.</span> Here is what the normal distribution looks like when the average is 0 and the SD is 1:
```{r}
m <- 0; s <- 1
norm_dist <- 
  data.frame(x = seq(-4, 4, len = 50) * s + m) |> 
  mutate(density = dnorm(x, m, s))
norm_dist |> 
  ggplot(aes(x,density)) + 
  geom_line()
```

For a list of numbers contained in a vector x, the average is defined as:
```
m <- sum(x) / length(x)
```
and the SD is defined as:
```
s <- sqrt(sum((x-mu)^2) / length(x))
```
which can be interpreted as the average distance between values and their average.

Let’s compute the values for the height for males which we will store in the object $x$:

```{r}
index <- heights$sex == "Male"
x <- heights$height[index]
str(x)
```
```{r}
y <- heights |>
  filter(sex == "Male") |>
  pull(height)
identical(x, y)
```

The pre-built functions mean and sd can be used here:
```{r}
options(digits = 3)
m <- mean(x)
s <- sd(x)
c(average = m, sd = s)
```

Advanced note: for reasons explained in statistics textbooks, `sd` divides by `length(x)-1` rather than `length(x)`. So `sd(x)` and `sqrt(sum((x-mu)^2) / length(x))` are practically equal when length(x) is large.

Here is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:

```{r}
norm_dist <- 
  data.frame(x = seq(-4, 4, len=50)*s + m) |> 
  mutate(density = dnorm(x, m, s))
heights |> 
  filter(sex == "Male") |> 
  ggplot(aes(height)) +
  geom_density(fill="#0099FF") +
  geom_line(aes(x, density),  
            data = norm_dist, 
            lwd=1.5) 
```

The normal distribution does appear to be quite a good approximation here. We now will see how well this approximation works at predicting the proportion of values within intervals.

## 12.4.1 Standard units

For data that is approximately normally distributed, it is convenient to think in terms of _standard units_.  The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value `x` from a vector `X`, we define the value of `x` in standard units as `z = (x - m)/s` with `m` and `s` the average and standard deviation of `X`, respectively. 

$$\mbox{Pr}(a < x \leq b) = \int_a^b \frac{1}{\sqrt{2\pi}s} e^{-\frac{1}{2}z^2} \, dx$$

Why is this convenient?

First look back at the formula for the normal distribution and note that what is being exponentiated is $-z^2/2$ with $z$ equivalent to $x$ in standard units. Because the maximum of  $e^{-z^2/2}$ is when $z=0$, this explains why the maximum of the distribution occurs at the average. It also explains the symmetry since $- z^2/2$ is symmetric around 0. Second, <span class="green">note that if we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average ($z=0$), one of the largest ($z \approx 2$), one of the smallest ($z \approx -2$), or an extremely rare occurrence ($z > 3$ or $z < -3$).</span> Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.

In R, we can obtain standard units using the function `scale` and see how many men are within 2 SDs from the average:


```{r}
z <- scale(x)
mean(abs(z) < 2)
```

The proportion is about 95%, which is what the normal distribution predicts. To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots.

**`scale`**
<blockquote id="scale">
`scale` is generic function whose default method centers and/or scales the columns of a numeric matrix.

`scale(x, center = TRUE, scale = TRUE`
</blockquote>

## 12.4.2 Quantile-quantile plots

A systematic way to assess how well the normal distribution fits the data is to check if the observed and predicted proportions match. In general, this is the approach of the quantile-quantile plot (QQ-plot).

First let's define the theoretical quantiles for the normal distribution. In statistics books <span class="green">we use the symbol $\Phi(x)$ to define the function that gives us the proportion of a standard normal distributed data that are smaller than $x$.</span> So, for example, $\Phi(-1.96) = 0.025$ and $\Phi(1.96) = 0.975$. In R, we can evaluate $\Phi$ using the <span id="pnorm">`pnorm`</span> function:

```{r}
pnorm(-1.96)
```

The inverse function $\Phi^{-1}(x)$ gives us the _theoretical quantiles_ for the normal distribution. So, for example, $\Phi^{-1}(0.975) = 1.96$. In R, we can evaluate the inverse of $\Phi$ using the <span id="qnorm">`qnorm`</span> function.

```{r}
qnorm(0.975)
```

<span class="green">Note that these calculations are for the standard normal distribution by default (mean = 0, standard deviation = 1), but we can also define these for any normal distribution.</span> We can do this using the `mean` and `sd` arguments in the `pnorm` and `qnorm` function. For example, we can use `qnorm` to determine quantiles of a distribution with a specific average and standard deviation
```{r}
qnorm(0.975,
      mean = 5,
      sd = 2)
```

For the normal distribution, all the calculations related to quantiles are done without data, thus the name __theoretical quantiles__. But quantiles can be defined for any distribution, including an empirical one. So if we have data in a vector $x$, we can define the quantile associated with any proportion $p$ as the $q$ for which the proportion of values below $q$ is $p$. Using R code, we can define `q` as the value for which `mean(x <= q) = p`. Notice that not all $p$ have a $q$ for which the proportion is exactly $p$. There are several ways of defining the best $q$ as discussed in the help for the `quantile` function. 

To give a quick example, for the male heights data, we have that:
```{r}
mean(x <= 69.5)
```

So about 50% are shorter or equal to 69 inches. This implies that if $p=0.50$ then $q=69.5$.

The idea of a QQ-plot is that if your data is well approximated by normal distribution then the quantiles of your data should be similar to the quantiles of a normal distribution. To construct a QQ-plot, we do the following:

1. Define a vector of $m$ proportions $p_1, p_2, \dots, p_m$.
2. Define a vector of quantiles $q_1, \dots, q_m$ for your data for the proportions $p_1, \dots, p_m$. We refer to these as the __sample quantiles__. 
3. Define a vector of theoretical quantiles for the proportions $p_1, \dots, p_m$ for a normal distribution with the same average and standard deviation as the data.
4. Plot the sample quantiles versus the theoretical quantiles.


Let's construct a QQ-plot using R code. Start by defining the vector of proportions.
```{r}
p <- seq(0.05, 0.95, 0.05)
```
To obtain the quantiles from the data, we can use the quantile function like this:
```{r}
sample_quantiles <- quantile(x, p)
sample_quantiles
```

To obtain the theoretical normal distribution quantiles with the corresponding average and SD, we use the `qnorm` function:

```{r}
theoretical_quantiles <- qnorm(p, mean(x), sd(x))
theoretical_quantiles
```

To see if they match or not, we plot them against each other and draw the identity line:
```{r}
qplot(theoretical_quantiles, sample_quantiles) +
  geom_abline()
```

Notice that this code becomes much cleaner if we use standard units:
```{r}
sample_quantiles <- quantile(z, p)
theoretical_quantiles <- qnorm(p) 
qplot(theoretical_quantiles, sample_quantiles) + 
  geom_abline()
```

The above code is included to help describe QQ-plots. However, in practice it is easier to use ggplot2 code:

```{r}
heights |>
  filter(sex == "Male") |>
  ggplot(aes(sample = scale(height))) +
  geom_qq() +
  geom_abline()
```

While for the illustration above we used 20 quantiles, the default from the `geom_qq` function is to use as many quantiles as data points.

Note that although here we used qqplots to compare an observed distribution to the mathematically defined normal distribution, QQ-plots can be used to compare any two distributions.

# 12.5 Percentiles

Before we move on, let’s define some terms that are commonly used in exploratory data analysis.

Percentiles are special cases of quantiles that are commonly used. The percentiles are the quantiles you obtain when setting the $p$ at $0.01,0.02,\dots,0.99$. We call, for example, the case of $p=0.25$ the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the median.

For the normal distribution the median and average are the same, but this is generally not the case.

Another special case that receives a name are the quartiles, which are obtained when setting $p=0.25,0.50, \text{ and }0.75$.

# 12.6 Boxplots

To introduce boxplots we will use a dataset of US murders by state. <span class="green">Suppose we want to summarize the murder rate distribution.</span> Using the techniques we have learned, we can quickly see that the normal approximation does not apply here:

```{r}
data(murders)
murders <- murders |>
  mutate(rate = total / population * 10^5)
library(gridExtra)
p1 <- 
  murders |>
  ggplot(aes(rate)) +
  geom_histogram(binwidth = 0.5, color = "black") +
  ggtitle("Histogram")
p2 <- 
  murders |> 
  ggplot(aes(sample=rate)) + 
  geom_qq(dparams=summarize(murders, mean=mean(rate), sd=sd(rate))) +
  geom_abline() + 
  ggtitle("QQ-plot")
grid.arrange(p1, p2, ncol = 2)
```

> NB. `geom_qq` requires an aesthetic named __sample__

In this case, the histogram above or a smooth density plot would serve as a relatively succinct summary.

Now suppose those used to receiving just two numbers as summaries ask us for a more compact numerical summary.

The boxplot provides a five-number summary composed of the range along with the quartiles (the 25th, 50th, and 75th percentiles). The boxplot often ignore outliers when computing the range and instead plot these as independent points. We provide a detailed explanation of outliers later. Finally, he suggested we plot these numbers as a “box” with “whiskers” like this:

```{r}
murders |>
  ggplot(aes("", rate)) +
  geom_boxplot() +
  coord_cartesian(xlim = c(0, 2)) +
  xlab("")
```
with the box defined by the 25% and 75% percentile and the whiskers showing the range. The distance between these two is called the _interquartile_ range. The two points are considered outliers by the default R function we used. The median is shown with a horizontal line. Today, we call these _boxplots_. 

From just this simple plot, we know that the median is about 2.5, that the distribution is not symmetric, and that the range is 0 to 5 for the great majority of states with two exceptions.


<blockquote id="geom_boxplot">
__`geom_boxplot`__ requires the following arguments:

- x or y
- lower or xlower
- upper or xupper
- middle or xmiddle
- ymin or xmin
- ymax or xmax
</blockquote>

__`coord_cartesian`__

<blockquote id="coord_cartesian">
The Cartesian coordinate system is the most familiar, and common, type of coordinate system. Setting limits on the coordinate system will zoom the plot (like you're looking at it with a magnifying glass), and will not change the underlying data like setting limits on a scale will.

`coord_cartesian(xlim = NULL, ylim = NULL, expand = TRUE, default = FALSE, clip = "on")`
</blockquote>

# 12.7 Stratification

In data analysis we often divide observations into groups based on the values of one or more variables associated with those observations. For example in the next section we divide the height values into groups based on a sex variable: females and males. We call this procedure __stratification__ and refer to the resulting groups as __strata__.

Stratification is common in data visualization because we are often interested in how the distribution of variables differs across different subgroups. We will see several examples throughout this part of the book.

## 12.7.1 Case study: describing student heights (continued)

Using the histogram, density plots, and QQ-plots, we have become convinced that the male height data is well approximated with a normal distribution. In this case, we report back to ET a very succinct summary: male heights follow a normal distribution with an average of 69.3 inches and a SD of 3.6 inches. With this information, ET will have a good idea of what to expect when he meets our male students. However, to provide a complete picture we need to also provide a summary of the female heights.

We learned that boxplots are useful when we want to quickly compare two or more distributions. Here are the heights for men and women:
```{r}
heights |> 
  ggplot(aes(x=sex, y=height, fill=sex)) +
  geom_boxplot()
```

The plot immediately reveals that males are, on average, taller than females. The standard deviations appear to be similar. But does the normal approximation also work for the female height data collected by the survey? We expect that they will follow a normal distribution, just like males. However, exploratory plots reveal that the approximation is not as useful:
```{r}
p1 <- heights |>
  ggplot(aes(height)) +
  geom_density(fill = "olivedrab2") +
  xlab("Women's heights")
p2 <- heights |>
  ggplot(aes(sample = scale(height))) +
  geom_qq() + geom_abline() +
  ylab("Standard Units")
grid.arrange(p1, p2, ncol = 2)
```

We see something we did not see for the males: the density plot has a second _bump_. Also, the QQ-plot shows that the highest points tend to be taller than expected by the normal distribution. Finally,  we also see five points in the QQ-plot that suggest shorter than expected heights for a normal distribution. When reporting back to ET, we might need to provide a histogram rather than just the average and standard deviation for the female heights. 

We have noticed what we didn't expect to see. If we look at other female height distributions, we do find that they are well approximated with a normal distribution. So why are our female students different? Is our class a requirement for the female basketball team? Are small proportions of females claiming to be taller than they are? Another, perhaps more likely, explanation is that in the form students used to enter their heights, `FEMALE` was the default sex and some males entered their heights, but forgot to change the sex variable. In any case, data visualization has helped discover a potential flaw in our data. 

Regarding the five smallest values, note that these values are:
```{r}
heights |>
  filter(sex == "Female") |>
  slice_min(height, n = 5) |>
  pull(height)
```

Because these are reported heights, a possibility is that the student meant to enter `5'1"`, `5'2"`, `5'3"` or `5'5"`. 

# 12.8 Exercises

1. Define variables containing the heights of males and females like this:

```{r}
library(dslabs)
data("heights")
male <- heights |> filter(sex == "Male") |> pull(height)
male2 <- heights$height[heights$sex == "Male"]
female <- heights$height[heights$sex == "Female"]
identical(male, male2)
```
How many measurements do we have for each?

```{r}
length(male); length(female)
```

2. Suppose we can't make a plot and want to compare the distributions side by side. We can't just list all the numbers. Instead, we will look at the percentiles. Create a five row table showing `female_percentiles` and `male_percentiles` with the 10th, 30th, 50th, 70th, & 90th percentiles for each sex. Then create a data frame with these two as columns.
```{r}
mp <- heights |>
  filter(sex == "Male") |>
  reframe(male_percentiles = quantile(height, c(10, 30, 50, 70, 90)/100))
fp <- 
  heights |>
  filter(sex == "Female") |>
  reframe(female_percentiles = quantile(height, c(10, 30, 50, 70, 90)/100))
data.frame(mp, fp)
```

3. Study the following boxplots showing population sizes by country:
```{r}
ds_theme_set()
data(gapminder)
tab <- 
  gapminder |> 
  filter(year == 2010) |> 
  group_by(continent) |> 
  select(continent, population)  
tab |> 
  ggplot(aes(x=continent, y=population/10^6)) + 
  geom_boxplot() + 
  scale_y_continuous()
  ylab("Population in millions")
```

Which continent has the country with the biggest population size? <span class="green">Asia</span>

4. What continent has the largest median population size? <span class="green">Africa</span>

5. What is median population size for Africa to the nearest million? <span class="green">11M<span>

6. What proportion of countries in Europe have populations below 14 million?

  a. 0.99
  b. <span class="green">0.75</span>
  c. 0.50
  d. 0.25
  
7. If we use a log transformation, which continent shown above has the largest interquartile range? <span class="green">Americas</span>
```{r}
tab |> 
  ggplot(aes(x=continent, y=population/10^6)) + 
  geom_boxplot() + 
  scale_y_continuous(trans = "log10", breaks = c(1,10,100,1000)) +
  ylab("Population in millions")
```

8. Load the height data set and create a vector `x` with just the male heights:

```{r}
data(heights)
x <- heights$height[heights$sex == "Male"]
```

What proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: use a logical operator and mean.

```{r}
mean(69 < x & x < 72)
```

9. Suppose all you know about the data is the average and the standard deviation. Use the normal approximation to estimate the proportion you just calculated. Hint: start by computing the average and standard deviation. Then use the pnorm function to predict the proportions.

```{r}
m <- mean(x)
s <- sd(x)
pnorm(x, m, s)
```

# 12.9 Robust summaries

## 12.9.1 Outliers

We previously described how boxplots show outliers, but we did not provide a precise definition. Here we discuss outliers, approaches that can help detect them, and summaries that take into account their presence.

Outliers are very common in real-world data anlysis. Data recording can be complex and it is common to observe data points generated in error. For example, an old monitoring device may read out nonsensical measurements before completely failing. Human error is also a source of outliers, in particular when data entry is done manually. An individual, for instance, may mistakenly enter their height in centimeters instead of inches or put the decimal in the wrong place.

How do we distinguish an outlier from measurements that were too big or too small simply due to expected variability? This is not always an easy question to answer, but we try to provide some guidance. Let’s begin with a simple case.

Suppose a colleague is charged with collecting demography data for a group of males. The data report height in feet and are stored in the object:
```{r}
library(tidyverse)
library(dslabs)
data(outlier_example)
str(outlier_example)
```

Our colleague uses the fact that heights are usually well approximated by a normal distribution and summarizes the data with average and standard deviation
```{r}
options(digits = 3)
mean(outlier_example); sd(outlier_example)
```

and writes a report on the interesting fact that this group of males is much taller than usual. The average height is over six feet tall! Using your data analysis skills, however, <span class="green">you notice something else that is unexpected: the standard deviation is over 7 feet.</span> Adding and subtracting two standard deviations, you note that 95% of this population will have heights between -9.489, 21.697 feet, which does not make sense. A quick plot reveals the problem:
```{r}
boxplot(outlier_example)
```

There appears to be at least one value that is nonsensical, since we know that a height of 180 feet is impossible. The boxplot detects this point as an outlier.

## 12.9.2 Median

When we have an outlier like this, the average can become very large. Mathematically, we can make the average as large as we want by simply changing one number: with `r length(outlier_example)` data points, we can increase the average by any amount $\Delta$ by adding $\Delta \times$ `r length(outlier_example)` to a single number. The median, defined as the value for which half the values are smaller and the other half are bigger, is robust to such outliers. No matter how large we make the largest point, the median remains the same. 

With this data the median is: 
```{r}
median(outlier_example)
```

which is about 5 feet and 9 inches.

The median is what boxplots display as a horizontal line.

## 12.9.3 The inter quartile range (IQR)

The box in boxplots is defined by the first and third quartile. These are meant to provide an idea of the variability in the data: 50% of the data is within this range. The difference between the 3rd and 1st quartile (or 75th and 25th percentiles) is referred to as the inter quartile range (IQR). As is the case with the median, this quantity will be robust to outliers as large values do not affect it. We can do some math to see that for normally distributed data, the IQR / 1.349 approximates the standard deviation of the data had an outlier not been present. We can see that this works well in our example since we get a standard deviation estimate of:

__`IQR()`__

<blockquote id="iqr">
computes interquartile range of the x values.

`IQR(x, na.rm = FALSE, type = 7)`

`type`: an integer selecting one of the many quantile algorithms

Note that this function computes the quartiles using the quantile function rather than following Tukey's recommendations, i.e., IQR(x) = quantile(x, 3/4) - quantile(x, 1/4).

For normally N(m,1)N(m,1) distributed XX, the expected value of IQR(X) is 2*qnorm(3/4) = 1.3490, i.e., for a normal-consistent estimate of the standard deviation, use IQR(x) / 1.349.
</blockquote>

```{r}
IQR(outlier_example) / 1.349
```

## 12.9.4 Tukey’s definition of an outlier

In R, points falling outside the whiskers of the boxplot are referred to as _outliers_. This definition of outlier was introduced by John Tukey. The top whisker ends at the 75th percentile plus 1.5 $\times$ IQR. Similarly the bottom whisker ends at the 25th percentile minus 1.5$\times$ IQR. If we define the first and third quartiles as $Q_1$ and $Q_3$, respectively, then an outlier is anything outside the range: 

$$[Q_1 - 1.5 \times (Q_3 - Q1), Q_3 + 1.5 \times (Q_3 - Q1)].$$ 

When the data is normally distributed, the standard units of these values are:

```{r}
q3 <- qnorm(0.75)
q1 <- qnorm(0.25)
iqr <- q3 - q1
r <- c(q1 - 1.5*iqr, q3 + 1.5*iqr)
r
```

Using the `pnorm` function, we see that `r round(pnorm(r[2]) - pnorm(r[1]),3)*100`% of the data falls in this interval.


```{r}
round(pnorm(r[2]) - pnorm(r[1]),3)*100
```

Keep in mind that this is not such an extreme event: if we have 1,000 data points that are normally distributed, we expect to see about 7 outside of this range. But these would not be outliers since we expect to see them under the typical variation.

If we want an outlier to be rarer, we can increase the 1.5 to a larger number. Tukey also used 3 and called these far out outliers. With a normal distribution, 100% of the data falls in this interval. This translates into about 2 in a million chance of being outside the range. <span class="green">In the geom_boxplot function, this can be controlled by the outlier.size argument, which defaults to 1.5.</span>

The 180 inches measurement is well beyond the range of the height data:

```{r}
max_height <- quantile(outlier_example, 0.75) + 3 * IQR(outlier_example)
max_height
```

If we take this value out, we can see that the data is in fact normally distributed as expected:

```{r}
x <- outlier_example[outlier_example < max_height]
qqnorm(x)
qqline(x)
```

## 12.9.5 Median absolute deviation {#mad}

Another way to robustly estimate the standard deviation in the presence of outliers is to use the median absolute deviation (MAD). To compute the MAD, we first compute the median, and then for each value we compute the distance between that value and the median. The MAD is defined as the median of these distances. For technical reasons not discussed here, this quantity needs to be multiplied by 1.4826 to assure it approximates the actual standard deviation. The `mad` function already incorporates this correction. For the height data, we get a MAD of:

```{r}
mad(outlier_example)
```

which is about 3 inches.

# 12.10 Exercises

We are going to use the __HistData__ package. If it is not installed you can install it like this:
```{r}
# install.packages("HistData")
```

Load the height data set and create a vector x with just the male heights used in Galton’s data on the heights of parents and their children from his historic research on heredity.
```{r}
library(HistData)
data(Galton)
x <- Galton$child
str(x)
```
1. Compute the average and median of these data.
```{r}
mean(x)
median(x)
```

2. Compute the median and median absolute deviation of these data.

```{r}
median(x)
mad(x)
```

3. Now suppose Galton made a mistake when entering the first value and forgot to use the decimal point. You can imitate this error by typing:

```{r}
x_with_error <- x
x_with_error[1] <- x_with_error[1]*10
mean(x_with_error) - mean(x)
```
```{r}
sd(x_with_error) - sd(x)
```

5. How many inches does the median grow after this mistake?

```{r}
median(x_with_error) - median(x)
```

6. How many inches does the MAD grow after this mistake?

```{r}
mad(x_with_error) - mad(x)
```

7. How could you use exploratory data analysis to detect that an error was made?

a. Since it is only one value out of many, we will not be able to detect this.
b. <span class="green">We would see an obvious shift in the distribution.</span>
c. <span class="green">A boxplot, histogram, or qq-plot would reveal a clear outlier.</span>
d. A scatterplot would show high levels of measurement error.

8. How much can the average accidentally grow with mistakes like this? Write a function called error_avg that takes a value k and returns the average of the vector x after the first entry changed to k. Show the results for k=10000 and k=-10000.

```{r}
error_avg <- function(x, k) {
  x_with_error <- x
  x_with_error[1] <- x_with_error[1]*k
  mean(x_with_error)
}
error_avg(x, 10000)
error_avg(x, -10000)
```
## 12.10.1 Case study: self-reported student heights

The heights we have been looking at are not the original heights reported by students. The original reported heights are also included in the dslabs package and can be loaded like this:
```{r}
library(dslabs)
data("reported_heights")
str(reported_heights)
```
```{r}
reported_heights <- 
  reported_heights |>
  mutate(original_heights = height,
         height = as.numeric(height))
str(reported_heights)
```

Note that we get a warning about NAs. This is because some of the self reported heights were not numbers. We can see why we get these:
```{r}
reported_heights |> filter(is.na(height)) |> head()
```

Some students self-reported their heights using feet and inches rather than just inches. Others used centimeters and others were just trolling. For now we will remove these entries:

```{r}
reported_heights <- filter(reported_heights, !is.na(height))
reported_heights |> filter(is.na(height)) |> head()
```

If we compute the average and standard deviation, we notice that we obtain strange results. The average and standard deviation are different from the median and MAD:
```{r}
reported_heights |>
  group_by(sex) |>
  summarise(average = mean(height), sd = sd(height),
            median = median(height), MAD = mad(height))
```

This suggests that we have outliers, which is confirmed by creating a boxplot:

```{r}
reported_heights |>
  ggplot(aes(sex, height)) +
  geom_boxplot()
```

We can see some rather extreme values. To see what these values are, we can quickly look at the largest values using the arrange function:
```{r}
reported_heights |>
  arrange(desc(height)) |>
  slice_max(height, n = 10)
```

The first seven entries look like strange errors. However, the next few look like they were entered as centimeters instead of inches. Since 184 cm is equivalent to six feet tall, we suspect that 184 was actually meant to be 72 inches.

<span class="green">We can review all the nonsensical answers by looking at the data considered to be far out by Tukey</span>:


```{r}
whisker <- 3*IQR(reported_heights$height)
max_height <- quantile(reported_heights$height, .75) + whisker
min_height <- quantile(reported_heights$height, .25) - whisker
reported_heights |> 
  filter(!between(height, min_height, max_height)) |> 
  select(original_heights) |>
  head(n=10) |> pull(original_heights)
```

Examining these heights carefully, we see two common mistakes: entries in centimeters, which turn out to be too large, and entries of the form `x.y` with `x` and `y` representing feet and inches, respectively, which turn out to be too small. Some of the even smaller values, such as 1.6, could be entries in meters.








































