---
title: "13 Probabilities"
output:
  html_document:
    df_print: paged
    css: "style.css"
    toc: true
---

[Book](http://rafalab.dfci.harvard.edu/dsbook/r-basics.html)

R commands in this chapter

|[`any`](#any)|
[`combinations`](#permutations)|
[`duplicated`](#duplicated)|
[`expand.grid`](#expand.grid)|
[`LETTERS`](#constants)|
[`letters`](#constants)|
[`month.abb`](#constants)|
[`month.name`](#constants)|
[`paste`](#paste)|
[`permutations`](#permutations)|
[`pi`](#constants)|
[`pnorm`](#pnorm)|
[`prop.table`](#prop.table)|
[`qnorm`](#qnorm)|
[`rep`](#rep)|
[`replicate`](#replicate)|
[`rnorm`](#rnorm)|
[`sample`](#sample)|
[`sapply`](#sapply)|
[`set.seed`](#set.seed)|
[`table`](#table)|

# 13.1 Discrete probability

We start by covering some basic principles related to categorical data. The subset of probability is referred to as __discrete probability__. It will help us understand the probability theory we will later introduce for numeric and continuous data, which is much more common in data science applications. Discrete probability is more useful in card games and therefore we use these as examples.

## 13.1.1 Relative frequency

The word probability is used in everyday language. Answering questions about probability is often hard, if not impossible. Here we discuss a mathematical definition of probability that does permit us to give precise answers to certain questions.

For example, if I have 2 red beads and 3 blue beads inside an urn44 (most probability books use this archaic term, so we do too) and I pick one at random, what is the probability of picking a red one? Our intuition tells us that the answer is 2/5 or 40%. A precise definition can be given by noting that there are five possible outcomes of which two satisfy the condition necessary for the event “pick a red bead”. Since each of the five outcomes has the same chance of occurring, we conclude that the probability is .4 for red and .6 for blue.

A more tangible way to think about the probability of an event is as the proportion of times the event occurs when we repeat the experiment an infinite number of times, independently, and under the same conditions.

## 13.1.2 Notation

We use the notation $Pr(A)$ to denote the probability of event $A$ happening. We use the very general term event to refer to things that can happen when something occurs by chance. In our previous example, the event was “picking a red bead”. In a political poll in which we call 100 likely voters at random, an example of an event is “calling 48 Democrats and 52 Republicans”.

In data science applications, we will often deal with continuous variables. These events will often be things like “is this person taller than 6 feet”. In this case, we write events in a more mathematical form: $X≥6$. We will see more of these examples later. Here we focus on categorical data.

## 13.1.3 Probability distribution

If we know the relative frequency of the different categories, defining a distribution for categorical outcomes is relatively straightforward. We simply assign a probability to each category. In cases that can be thought of as beads in an urn, for each bead type, their proportion defines the distribution.

If we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% undecided, and 2% Green Party, these proportions define the probability for each group. The probability distribution is:

||||
|-------------------------|---|----|
|Pr(picking a Republican)|=|0.44|
|Pr(picking a Democrat)|=|0.44|
|Pr(picking an undecided)|=|0.10|
|Pr(picking a Green)|=|0.02|

# 13.2 Monte Carlo simulation

__Key points__

- The probability of an event is the proportion of times the event occurs when we repeat the experiment independently under the same conditions.
$$
\mbox{Pr}(A) =  \mbox{probability of event A}
$$
- An *event* is defined as an outcome that can occur when when something happens by chance.
- We can determine probabilities related to discrete variables (picking a red bead, choosing 48 Democrats and 52 Republicans from 100 likely voters) and continuous variables (height over 6 feet).
- Monte Carlo simulations model the probability of different outcomes by repeating a random process a large enough number of times that the results are similar to what would be observed if the process were repeated forever.
- The `sample()` function draws random outcomes from a set of options.
- The `replicate()` function repeats lines of code a set number of times. It is used with `sample()` and similar functions to run Monte Carlo simulations.

Computers provide a way to actually perform the simple random experiment described above: pick a bead at random from a bag that contains three blue beads and two red ones. Random number generators permit us to mimic the process of picking at random.

An example is the `sample function in` R. 

<blockquote id="sample">
__`sample`__

sample takes a sample of the specified size from the elements of x using either with or without replacement.

`sample(x, size, replace = FALSE, prob = NULL)`

prob

The optional prob argument can be used to give a vector of weights for obtaining the elements of the vector being sampled. They need not sum to one, but they should be non-negative and not all zero. 
</blockquote>

We demonstrate its use in the code below. First, we use the function `rep` to generate the urn:

<blockquote id="rep">
__`rep()`__

rep replicates the values in x. It is a generic function, and the (internal) default method is described here.

`rep(x, ...)`

- $x$: a vector (of any mode including a list) or a factor
- times: an integer-valued vector giving the (non-negative) number of times to repeat each element if of length length(x), or to repeat the whole vector if of length 1. Negative or NA values are an error. A double vector is accepted, other inputs being coerced to an integer or double vector.
- length.out: non-negative integer. The desired length of the output vector. Other inputs will be coerced to a double vector and the first element taken. Ignored if NA or invalid.
- each: non-negative integer. Each element of `x` is repeated `each` times. Other inputs will be coerced to an integer or double vector and the first element taken. Treated as 1 if NA or invalid.
</blockquote>


```{r}
beads <- rep(c("red", "blue"), times = c(2,3))
beads
```
```{r}
sample(beads, 1)
```
This line of code produces one random outcome. We want to repeat this experiment an infinite number of times, but it is impossible to repeat forever. Instead, <span class="blue">we repeat the experiment a large enough number of times to make the results practically equivalent to repeating forever.</span> This is an example of a Monte Carlo simulation.

Much of what mathematical and theoretical statisticians study, which we do not cover in this book, relates to providing rigorous definitions of “practically equivalent” as well as studying how close a large number of experiments gets us to what happens in the limit. Later in this section, we provide a practical approach to deciding what is “large enough”.

To perform our first Monte Carlo simulation, we use the `replicate` function, which permits us to repeat the same task any number of times. 

<blockquote id="replicate">
__`replicate`__

replicate is a wrapper for the common use of sapply for repeated evaluation of an expression (which will usually involve random number generation).

`replicate(n, expr, simplify = "array")`

simplify

logical or character string; should the result be simplified to a vector, matrix or higher dimensional array if possible? 
</blockquote>

Here, we repeat the random event $B=10,000$ times:

```{r}
B <- 10000
events <- replicate(B, sample(beads, 1))
```

We can now see if our definition actually is in agreement with this Monte Carlo simulation approximation. We can use <span id="table">`table`</span> to see the distribution:

```{r}
tab <- table(events)
tab
```
and <span id="prop.table">`prop.table`</span> gives us the proportions:
```{r}
prop.table(tab)
```
The numbers above are the estimated probabilities provided by this Monte Carlo simulation. Statistical theory, not covered here, tells us that as $B$ gets larger, the estimates get closer to 3/5=.6 and 2/5=.4.

Although this is a simple and not very useful example, we will use Monte Carlo simulations to estimate probabilities in cases in which it is harder to compute the exact ones. Before delving into more complex examples, we use simple ones to demonstrate the computing tools available in R.

## 13.2.1 Setting the random seed {#set.seed}

```{r}
set.seed(1986)
```

__Important note on seeds in R 3.5 versus R 3.6 and later__

When R updated to version 3.6 in early 2019, the default method for setting the seed changed. This means that exercises, videos, textbook excerpts and other code you encounter online may yield a different result based on your version of R.

If you are running R 3.6 or later, you can revert to the original seed setting behavior by adding the argument sample.kind="Rounding". For example:
```
set.seed(1, sample.kind="Rounding")
```

## 13.2.2 With and without replacement

The function `sample` has an argument that permits us to pick more than one element from the urn. However, by default, this selection occurs without replacement: after a bead is selected, it is not put back in the bag. Notice what happens when we ask to randomly select five beads:
```{r}
sample(beads, 5)
sample(beads, 5)
sample(beads, 5)
```
This results in rearrangements that always have three blue and two red beads. If we ask that six beads be selected, we get an error:
```{r}
#sample(beads, 6)
```
`Error in sample.int(length(x), size, replace, prob) : cannot take a sample larger than the population when 'replace = FALSE'

However, the `sample` function can be used directly, without the use of `replicate`, to repeat the same experiment of picking 1 out of the 5 beads, continually, under the same conditions. To do this, we sample __with replacement__: return the bead back to the urn after selecting it. We can tell sample to do this by changing the `replace` argument, which defaults to `FALSE`, to `replace = TRUE`:

```{r}
options(digits = 3)
set.seed(1986)
events <- sample(beads, B, replace = TRUE)
prop.table(table(events))
```
Not surprisingly, we get results very similar to those previously obtained with `replicate`.

# 13.3 Independence

__Key points__

- Conditional probabilities compute the probability that an event occurs given information about dependent events. For example, the probability of drawing a second king given that the first draw is a king is:
$$
\mbox{Pr(Card 2 is a king}\mid \mbox{Card 1 is a king)} = 3/51
$$
- If two events  and  are independent, $\mbox{Pr}(A \mid B) = \mbox{Pr}(A)$.
- To determine the probability of multiple events occurring, we use the __multiplication__ rule.

__Equations__

- The multiplication rule for independent events is:
$$
\mbox{Pr}(A \mbox{ and }B \mbox{ and }C) = \mbox{Pr}(A) \times \mbox{Pr}(B) \times \mbox{Pr}(C)
$$
The multiplication rule for dependent events considers the conditional probability of both events occurring:
$$
\mbox{Pr}(A \mbox{ and }B) = \mbox{Pr}(A)\times\mbox{Pr}(B \mid A)
$$
We can expand the multiplication rule for dependent events to more than 2 events:
$$
\mbox{Pr}(A \mbox{ and }B \mbox{ and }C) = \mbox{Pr}(A) \times \mbox{Pr}(B \mid A) \times \mbox{Pr}(C \mid A \mbox{ and } B)
$$


We say two events are independent if the outcome of one does not affect the other. The classic example is coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement. In the example above, the probability of red is 0.40 regardless of previous draws.

Many examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities: Ace, Deuce, Three, $\dots$, Ten, Jack, Queen, King, and Ace. Now if we deal a King for the first card, and don’t replace it into the deck, the probabilities of a second card being a King is less because there are only three Kings left: the probability is 3 out of 51. These events are therefore __not independent__: the first outcome affected the next one.

To see an extreme case of non-independent events, consider our example of drawing five beads at random __without__ replacement:

```{r}
set.seed(1986)
x <- sample(beads, 5)
```

If you have to guess the color of the first bead, you will predict blue since blue has a 60% chance. But if I show you the result of the last four outcomes:
```{r}
x[2:5]
```
would you still guess blue? Of course not. Now you know that the probability of red is 1 since the only bead left is red. The events are not independent, so the probabilities change.

# 13.4 Conditional probabilities

When events are not independent, _conditional probabilities_ are useful. We already saw an example of a conditional probability: we computed the probability that a second dealt card is a King given that the first was a King. In probability, we use the following notation:

$$
\mbox{Pr}(\mbox{Card 2 is a king} \mid \mbox{Card 1 is a king}) = 3/51
$$

We use the $\mid$ as shorthand for "given that" or "conditional on". 

When two events, say $A$ and $B$, are independent, we have:

$$
\mbox{Pr}(A \mid B) = \mbox{Pr}(A) 
$$

This is the mathematical way of saying: the fact that $B$ happened does not affect the probability of $A$ happening. In fact, this can be considered the mathematical definition of independence.


# 13.5 Addition and multiplication rules

# 13.5.1 Multiplication rule

If we want to know the probability of two events, say $A$ and $B$, occurring, we can use the multiplication rule:

$$ 
\mbox{Pr}(A \mbox{ and } B) = \mbox{Pr}(A)\mbox{Pr}(B \mid A)
$$
Let's use Blackjack as an example. In Blackjack, you are assigned two random cards. After you see what you have, you can ask for more. The goal is to get closer to 21 than the dealer, without going over. Face cards are worth 10 points and Aces are worth 11 or 1 (you choose).

So, in a Blackjack game, to calculate the chances of getting a 21 by drawing an Ace and then a face card, we compute the probability of the first being an Ace and multiply by the probability of drawing a face card or a 10 given that the first was an Ace: $1/13 \times 16/51 \approx 0.025$

The multiplication rule also applies to more than two events. We can use induction to expand for more events:

$$ 
\mbox{Pr}(A \mbox{ and } B \mbox{ and } C) = \mbox{Pr}(A)\mbox{Pr}(B \mid A)\mbox{Pr}(C \mid A \mbox{ and } B)
$$

## 13.5.2 Multiplication rule under independence

When we have independent events, then the multiplication rule becomes simpler:

$$ 
\mbox{Pr}(A \mbox{ and } B \mbox{ and } C) = \mbox{Pr}(A)\mbox{Pr}(B)\mbox{Pr}(C)
$$

But we have to be very careful before using this since assuming independence can result in very different and incorrect probability calculations when we don't actually have independence.

As an example, imagine a court case in which the suspect was described as having a mustache and a beard. The defendant has a mustache and a beard and the prosecution brings in an "expert" to testify that 1/10 men have beards and 1/5 have mustaches, so using the multiplication rule we conclude that only $1/10 \times 1/5$ or 0.02 have both. 

But to multiply like this we need to assume independence! Say the conditional probability of a man having a mustache conditional on him having a beard is .95. So the correct calculation probability is much higher: $1/10 \times 95/100 = 0.095$.

The multiplication rule also gives us a general formula for computing conditional probabilities:


$$ 
\mbox{Pr}(B \mid A) = \frac{\mbox{Pr}(A \mbox{ and } B)}{ \mbox{Pr}(A)}
$$

To illustrate how we use these formulas and concepts in practice, we will use several examples related to card games.

## 13.5.3 Addition rule

The addition rule tells us that:

$$
\mbox{Pr}(A \mbox{ or } B) = \mbox{Pr}(A) + \mbox{Pr}(B) - \mbox{Pr}(A \mbox{ and } B)
$$

This rule is intuitive: think of a Venn diagram. If we simply add the probabilities, we count the intersection twice so we need to substract one instance.

```{r}
# install.packages("VennDiagram")
# install.packages("rafalib")
library(VennDiagram)
rafalib::mypar()
grid.newpage()
tmp <- draw.pairwise.venn(22, 20, 11, category = c("A", "B"), 
                   lty = rep("blank", 2), 
                   fill = c("lightblue", "pink"), 
                   alpha = rep(0.5, 2),  
                   cat.dist = rep(0.025, 2), 
                   cex=0, 
                   cat.cex = rep(2.5,2))
```

<blockquote>
__`mypar`__

Called without arguments, this function optimizes graphical parameters for the RStudio plot window. bigpar uses big fonts which are good for presentations.
</blockquote>

# 13.6 Combinations and permutations

__Key points__

- `paste()` joins two strings and inserts a space in between.
- `expand.grid()` gives the combinations of 2 vectors or lists.
- `permutations(n,r)` from the __gtools__ package lists the different ways that r items can be selected from a set of n options when order matters.
- `combinations(n,r)` from the __gtools__ package lists the different ways that r items can be selected from a set of n options when order does not matter.

In our very first example, we imagined an urn with five beads. As a reminder, to compute the probability distribution of one draw, we simply listed out all the possibilities. There were 5 and so then, for each event, we counted how many of these possibilities were associated with the event. The resulting probability of choosing a blue bead is 3/5 because out of the five possible outcomes, three were blue.

For more complicated cases, the computations are not as straightforward. For instance, what is the probability that if I draw five cards without replacement, I get all cards of the same suit, what is known as a “flush” in poker? In a discrete probability course you learn theory on how to make these computations. Here we focus on how to use R code to compute the answers.

First, let’s construct a deck of cards. For this, we will use the <span id="expand.grid">__`expand.grid`__</span> and <span id="paste">__`paste`__</span> functions. We use `paste` to create strings by joining smaller strings. To do this, we take the number and suit of a card and create the card name like this:
```{r}
number <- "Three"
suit <- "Hearts"
paste(number, suit)
```
`paste` also works on pairs of vectors performing the operation element-wise:
```{r}
paste(letters[1:5], as.character(1:5), sep = ":", collapse = ",")
```
<blockquote id="constants">
__Built-in Constants__

- LETTERS: the 26 upper-case letters of the Roman alphabet;
- letters: the 26 lower-case letters of the Roman alphabet;
- month.abb: the three-letter abbreviations for the English month names;
- month.name: the English names for the months of the year;
- pi: the ratio of the circumference of a circle to its diameter.

</blockquote>


The function `expand.grid` gives us all the combinations of entries of two vectors. For example, if you have blue and black pants and white, grey, and plaid shirts, all your combinations are:

```{r}
expand.grid(pants = c("blue", "black"),
            shirt = c("white", "grey", "plaid"))
```
Generate a deck of cards
```{r}
ranks <- c("Ace", "Two", "Three", "Four", "Five", "Six", "Seven",
           "Eight", "Nine", "Ten", "Jack", "Queen", "King")
suits <- c("Hearts", "Spades", "Diamonds", "Clubs")
deck <- expand.grid(rank = ranks, suit = suits)
deck <- paste(deck$rank, deck$suit)
```

With the deck constructed, <span class="blue">we can double check that the probability of a King in the first card is 1/13 by computing the proportion of possible outcomes that satisfy our condition</span>:

```{r}
kings <- paste("King", suits)
mean(deck %in% kings)
```

Now, how about the conditional probability of the second card being a King given that the first was a King? Earlier, we deduced that if one King is already out of the deck and there are 51 left, then this probability is 3/51. Let’s confirm by listing out all possible outcomes.

To do this, we can use the <span id="permutations">`permutations`</span> function from the __gtools__ package. For any list of size `n`, this function computes all the different combinations we can get when we select `r` items. Here are all the ways we can choose two numbers from a list consisting of 1,2,3:
```{r}
# install.packages("gtools")
library(gtools)
permutations(3, 2)
```
<blockquote id="permutations">
`combinations(n, r, v = 1:n, set = TRUE, repeats.allowed = FALSE)`

`permutations(n, r, v = 1:n, set = TRUE, repeats.allowed = FALSE)`

- n: Size of the source vector
- r: Size of the target vectors
- v: Source vector. Defaults to 1:n
- set: Logical flag indicating whether duplicates should be removed from the source vector v. Defaults to TRUE.
- repeats.allowed: Logical flag indicating whether the constructed vectors may include duplicated values. Defaults to FALSE.
</blockquote>

Notice that the order matters here: 3,1 is different than 1,3. Also, note that (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again.

Optionally, we can add a vector. If you want to see five random seven digit phone numbers out of all possible phone numbers (without repeats), you can type:

```{r}
all_phone_numbers <- permutations(10, 7, v = 0:9)
n <- nrow(all_phone_numbers)
set.seed(1986)
index <- sample(n, 5)
all_phone_numbers[index,]
```
Instead of using the numbers 1 through 10, the default, it uses what we provided through v: the digits 0 through 9.

<span class="blue">To compute all possible ways we can choose two cards when the order matters, we type:</span>
```{r}
hands <- permutations(52, 2, v = deck)
str(hands)
```
With a matrix we can get the first and second cards like this:
```{r}
first_card <- hands[,1]
second_card <- hands[,2]
```

Now the cases for which the first hand was a King can be computed like this:

```{r}
kings <- paste("King", suits)
sum(first_card %in% kings)
```
<span class="blue">To get the conditional probability, we compute what fraction of these have a King in the second card:</span>
```{r}
sum(first_card %in% kings & second_card %in% kings) / 
  sum(first_card %in% kings)
```
which is exactly 3/51, as we had already deduced. Notice that the code above is equivalent to:
```{r}
mean(first_card%in%kings & second_card%in%kings) / mean(first_card%in%kings)
```
which uses `mean` instead of `sum` and is an R version of: 

$$
\frac{\mbox{Pr}(A \mbox{ and } B)}{ \mbox{Pr}(A)}
$$

How about if the order doesn’t matter? For example, in Blackjack if you get an Ace and a face card in the first draw, it is called a Natural 21 and you win automatically. If we wanted to compute the probability of this happening, we would enumerate the combinations, not the permutations, since the order does not matter.
```{r}
combinations(3, 2)
```
In the second line, the outcome does not include (2,1) because (1,2) already was enumerated. The same applies to (3,1) and (3,2).

<span class="blue">So to compute the probability of a Natural 21 in Blackjack, we can do this:</span>

```{r}
aces <- paste("Ace", suits)
facecard <- c("Ten", "Jack", "Queen", "King")
facecard <- expand.grid(number = facecard, suit = suits)
facecard <- paste(facecard$number, facecard$suit)

hands <- combinations(52, 2, v = deck)
mean(hands[,1] %in% aces & hands[,2] %in% facecard)
```
n the last line, we assume the Ace comes first. This is only because we know the way combination enumerates possibilities and it will list this case first. But to be safe, we could have written this and produced the same answer:

```{r}
mean((hands[,1] %in% aces & hands[,2] %in% facecard) | 
    (hands[,2] %in% aces & hands[,1] %in% facecard))
```

## 13.6.1 Monte Carlo example

Instead of using `combinations` to deduce the exact probability of a Natural 21, we can use a Monte Carlo to estimate this probability. In this case, we draw two cards over and over and keep track of how many 21s we get. We can use the function sample to draw two cards without replacements:

```{r}
hand <- sample(deck, 2)
hand
```
And then check if one card is an Ace and the other a face card or a 10. Going forward, we include 10 when we say face card. Now we need to check both possibilities:
```{r}
(hands[1] %in% aces & hands[2] %in% facecard) | 
  (hands[2] %in% aces & hands[1] %in% facecard)
```
If we repeat this 10,000 times, we get a very good approximation of the probability of a Natural 21.

Let’s start by writing a function that draws a hand and returns TRUE if we get a 21. The function does not need any arguments because it uses objects defined in the global environment.

```{r}
blackjack <- function(){
   hand <- sample(deck, 2)
  (hand[1] %in% aces & hand[2] %in% facecard) | 
    (hand[2] %in% aces & hand[1] %in% facecard)
}
```

Here we do have to check both possibilities: Ace first or Ace second because we are not using the combinations function. The function returns `TRUE` if we get a 21 and `FALSE` otherwise:

```{r}
blackjack()
```
Now we can play this game, say, 10,000 times:
```{r}
B <- 10000
results <- replicate(B, blackjack())
mean(results)
```

# 13.7 Examples

In the 1970s, there was a game show called “Let’s Make a Deal” and Monty Hall was the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door there was a prize. The other doors had a goat behind them to show the contestant they had lost. After the contestant picked a door, before revealing whether the chosen door contained a prize, Monty Hall would open one of the two remaining doors and show the contestant there was no prize behind that door. Then he would ask “Do you want to switch doors?” What would you do?

We can use probability to show that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3! This seems counterintuitive. Many people incorrectly think both chances are 1 in 2 since you are choosing between 2 options. You can watch a detailed mathematical explanation on Khan Academy45 or read one on Wikipedia46. Below we use a Monte Carlo simulation to see which strategy is better. Note that this code is written longer than it should be for pedagogical purposes.

## Monty Hall Problem

Let’s start with the stick strategy:

```{r}
doors <- as.character(1:3)
prize <- sample(c("car", "goat", "goat"))
prize_door <- doors[prize == "car"]
my_pick  <- sample(doors, 1)
sample(doors[!doors %in% c(my_pick, prize_door)])
```

```{r}
B <- 10000
monty_hall <- function(strategy){
  doors <- as.character(1:3)
  prize <- sample(c("car", "goat", "goat"))
  prize_door <- doors[prize == "car"]
  my_pick  <- sample(doors, 1)
  show <- sample(doors[!doors %in% c(my_pick, prize_door)],1)
#  stick <- my_pick
#  stick == prize_door
  switch <- doors[!doors%in%c(my_pick, show)]
#  choice <- ifelse(strategy == "stick", stick, switch)
  choice <- ifelse(strategy == "stick", my_pick, switch)
  choice == prize_door
}
stick <- replicate(B, monty_hall("stick"))
mean(stick)
switch <- replicate(B, monty_hall("switch"))
mean(switch)
```
As we write the code, we note that the lines starting with `my_pick` and `show` have no influence on the last logical operation when we stick to our original choice anyway. From this we should realize that the chance is 1 in 3, what we began with. When we switch, the Monte Carlo estimate confirms the 2/3 calculation. This helps us gain some insight by showing that we are removing a door, show, that is definitely not a winner from our choices. We also see that unless we get it right when we first pick, you win: 1 - 1/3 = 2/3.
```{r}

```

## 13.7.2 Birthday problem

Suppose you are in a classroom with 50 people. If we assume this is a randomly selected group of 50 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do this later. Here we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29. This actually doesn’t change the answer much.

First, note that birthdays can be represented as numbers between 1 and 365, so a sample of 50 birthdays can be obtained like this:

```{r}
n <- 50
bdays <- sample(1:365, n, replace = TRUE)
```
To check if in this particular set of 50 people we have at least two with the same birthday, we can use the function duplicated, which returns TRUE whenever an element of a vector is a duplicate. Here is an example:
```{r}
duplicated(c(1,2,3,1,4,3,5))
```

So to check if two birthdays were the same, we simply use the any and duplicated functions like this:
```{r}
any(duplicated(bdays))
```

<blockquote id="duplicated">
__`duplicated`__

Determines which elements of a vector or data frame are duplicates of elements with smaller subscripts, and returns a logical vector indicating which elements (rows) are duplicates.

`duplicated(x, incomparables = FALSE, ...)`

- incomparables: a vector of values that cannot be compared. `FALSE` is a special value, meaning that all values can be compared, and may be the only value accepted for methods other than the default. It will be coerced internally to the same type as `x`.
- fromLast: logical indicating if duplication should be considered from the reverse side, i.e., the last (or rightmost) of identical elements would correspond to duplicated = FALSE.
- nmax: the maximum number of unique items expected (greater than one).
</blockquote>

<blockquote id="any">
__`any()`__

Given a set of logical vectors, is at least one of the values true?

`any(..., na.rm = FALSE)`
</blockquote>

In this case, we see that it did happen. At least two people had the same birthday.

To estimate the probability of a shared birthday in the group, we repeat this experiment by sampling sets of 50 birthdays over and over:

```{r}
B <- 10000
same_birthday <- function(n) {
  bdays <- sample(1:365, n, replace = TRUE)
  any(duplicated(bdays))
}
results <- replicate(B, same_birthday(50))
mean(results)
```

Were you expecting the probability to be this high?

People tend to underestimate these probabilities. To get an intuition as to why it is so high, think about what happens when the group size is close to 365. At this stage, we run out of days and the probability is one.

Say we want to use this knowledge to bet with friends about two people having the same birthday in a group of people. When are the chances larger than 50%? Larger than 75%?

Let’s create a look-up table. We can quickly create a function to compute this for any group size:

```{r}
compute_prob <- function(n, B = 10000) {
  results <- replicate(B, same_birthday(n))
  mean(results)
}
```
Using the function <span id="sapply">`sapply`</span>, we can perform element-wise operations on any function:

__`sapply()`__

- Some functions automatically apply element-wise to vectors, such as `sqrt()` and `*`.
- However, other functions do not operate element-wise by default. This includes functions we define ourselves.
- The function `sapply(x, f)` allows any other function `f` to be applied element-wise to the vector `x`.
- The probability of an event happening is 1 minus the probability of that event not happening:
$$
\mbox{Pr(event)} = 1 - \mbox{Pr(no event)}
$$

We can compute the probability of shared birthdays mathematically:

$$
\mbox{Pr(shared birthdays)} = 1 -\mbox{Pr(no shared birthdays)} =\\ 1 - (1 \times  \frac{364}{365} \times \frac{363}{365} \times ... \times \frac{365-n+1}{365})
$$

```{r}
n <- seq(1, 60)
prob <- sapply(n, compute_prob)
```

We can now make a plot of the estimated probabilities of two people having the same birthday in a group of size $n$:

```{r}
library(tidyverse)
# qplot(n, prob)  Depracated
data.frame(n = n, prob = prob) |>
  ggplot(aes(n, prob)) +
  geom_point()
```

Now let's compute the exact probabilities rather than use Monte Carlo approximations. Not only do we get the exact answer using math, but the computations are much faster since we don't have to generate experiments.  


To make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening. For this, we use the multiplication rule.

Let's start with the first person. The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one, is 364/365. Then, given that the first two people have unique birthdays, person 3 is left with 363 days to choose from. We continue this way and find the chances of all 50 people having a unique birthday is:

$$
1 \times \frac{364}{365}\times\frac{363}{365} \dots \frac{365-n + 1}{365}
$$

We can write a function that does this for any number:

```{r}
exact_prob <- function(n) {
  prob_unique <- seq(365, 365 - n + 1) / 365
  1 - prod(prob_unique)
}
eprob <- sapply(n, exact_prob)
data.frame(n = n, eprob = eprob) |>
  ggplot(aes(n, eprob)) +
  geom_line(col = "red") +
  geom_point(aes(n, prob))
```

This plot shows that the Monte Carlo simulation provided a very good estimate of the exact probability. Had it not been possible to compute the exact probabilities, we would have still been able to accurately estimate the probabilities.

# 13.8 Infinity in practice

The theory described here requires repeating experiments over and over forever. In practice we can’t do this. In the examples above, we used $B=10,000$
  Monte Carlo experiments and it turned out that this provided accurate estimates. The larger this number, the more accurate the estimate becomes until the approximaton is so good that your computer can’t tell the difference. But in more complex calculations, 10,000 may not be nearly enough. Also, for some calculations, 10,000 experiments might not be computationally feasible. In practice, we won’t know what the answer is, so we won’t know if our Monte Carlo estimate is accurate. <span class="blue">We know that the larger $B$, the better the approximation. But how big do we need it to be? This is actually a challenging question and answering it often requires advanced theoretical statistics training.</span>

One practical approach we will describe here is to <span class="blue">check for the stability of the estimate.</span> The following is an example with the birthday problem for a group of 25 people.

```{r}
B <- 10^seq(1, 5, len = 100)
compute_prob <- function(B, n = 25) {
  same_day <- replicate(B, same_birthday(n))
  mean(same_day)
}
prob <- sapply(B, compute_prob)
data.frame(logB = log10(B), prob) |>
  ggplot(aes(logB, prob)) +
  xlab("log10(B)") +
  geom_line()
```

In this plot, <span class="blue">we can see that the values start to stabilize (that is, they vary less than .01) around 1000</span>. Note that the exact probability, which we know in this case, is 0.569.

# 13.9 Exercises

1. One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?

```{r}
# urn <- c(rep("cyan", 3), rep("magenta", 5), rep("yellow", 7))
urn <- rep(c("cyan", "magenta", "yellow"), times = c(3, 5, 7))
urn
```
```{r}
sample(urn, 1)
```
```{r}
B <- 10000
events <- replicate(B, sample(urn, 1))
tab <- table(events)
tab
```
```{r}
prop.table(tab)
```
```{r}
mean(events %in% "cyan")
```
<span class="blue">Answer 20%</span>

2. What is the probability that the ball will not be cyan?
```{r}
mean(!events %in% "cyan")
```
<span class="blue">80%</span>

3. Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?

```{r}
events <- replicate(B, sample(urn, 2))
mean(events[1,] %in% "cyan" & !events[2,] %in% "cyan")
```
<span class="blue">17%</span>

4. Now repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?

```{r}
events <- replicate(B, sample(urn, 2, replace = TRUE))
mean(events[1,] %in% "cyan" & !events[2,] %in% "cyan")
```
<span class="blue">16%</span>

5. Two events $A$ and $B$ are independent if $Pr(A and B)=Pr(A)P(B). Under which situation are the draws independent?

a. You don’t replace the draw.
b. <span class="blue">You replace the draw.</span>
c. Neither
d. Both

6. Say you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow?
```{r}
events <- replicate(B, sample(urn, 5))
mean(events[1:4,] %in% "yellow" & events[5,] %in% "yellow")
```
```{r}
mean(events[1,] %in% "yellow" & 
       events[2,] %in% "yellow" & 
       events[3,] %in% "yellow" & 
       events[4,] %in% "yellow" & 
       events[4,] %in% "yellow")

```
<span class="blue">2.8%</span>

7. If you roll a 6-sided die six times, what is the probability of not seeing a 6?

```{r}
die <- as.character(c(1,2,3,4,5,6))
sample(die, 6, replace = TRUE)
```
```{r}
events <- replicate(B, sample(die, 6, replace = TRUE))
sum(!events == "6")
```
```{r}
(length(events) - sum(!events == "6")) / length(events)
```
<span class="blue">17%</span>

8. Two teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics win at least one game?

```{r}
events <- replicate(B, sample(c(0,1), 
                              4, 
                              replace = TRUE, 
                              prob = c(0.6, 0.4)))
mean(events == 1)
```
<span class="blue">40%</span>

9. Create a Monte Carlo simulation to confirm your answer to the previous problem. Use B <- 10000 simulations. Hint: use the following code to generate the results of the first four games:

```{r}
celtic_wins <- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4))
```
The Celtics must win one of these 4 games.

10. Two teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series?

```{r}
celtic_wins <- sample(c(0,1), 7, replace = TRUE, prob = c(0.5, 0.5))
str(celtic_wins)
all_perms <- permutations(2, 7, v = c(0,1), repeats.allowed = TRUE)
mean(all_perms[,1] == 0 & 
       (all_perms[,2] + all_perms[,3] + all_perms[,4] +
          all_perms[,5] + all_perms[,6] + all_perms[,7]) >= 4)
```
<span class="green">17%</span>

11. Confirm the results of the previous question with a Monte Carlo simulation.

```{r}
simulated_games <- sample(c("lose","win"), 4, replace = TRUE, prob = c(0.6, 0.4))
```

The variable 'B' specifies the number of times we want the simulation to run. Let's run the Monte Carlo simulation 10,000 times.
```{r}
B <- 10000
```

Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
```{r}
set.seed(1)
```

Create an object called `celtic_wins` that replicates two steps for B iterations: (1) generating a random four-game series `simulated_games` using the example code, then (2) determining whether the simulated series contains at least one win for the Celtics. Put these steps on separate lines.
```{r}
celtic_wins <- replicate(B, {
  simulated_games <- sample(c("lose","win"), 4, replace = TRUE, prob = c(0.6, 0.4))
  any(simulated_games=="win")
})
```

Calculate the frequency out of B iterations that the Celtics won at least one game. Print your answer to the console. 
```{r}
mean(celtic_wins)
```

# 13.10 Continuous probability

In Section 12.2.2, we explained why when summarizing a list of numeric values, such as heights, it is not useful to construct a distribution that defines a proportion to each possible outcome. Similarly, for a random variable that can take any value in a continuous set, it impossible to assign a positive probabilities to the infinite number of possible values. Here we describe how we mathematically define distributions for continuos random variables and useful approximations often used in data analysis.

## 13.10.1 Cumulative distribution functions

We used the heights of adult male students as an example
```{r}
library(tidyverse)
library(dslabs)
data(heights)
x <- 
  heights %>% 
  filter(sex == "Male") %>% 
  pull(height)
```
and <span class="blue">defined the empirical cumulative distribution function (eCDF) as</span>
```{r}
F <- function(a) mean(x<=a)
```
<span class="blue">which, for any value a, gives the proportion of values in the list `x` that are smaller or equal than `a`.</span>

Let’s connect the eCDF to probability by asking: if I pick one of the male students at random, what is the chance that he is taller than 70.5 inches? Because every student has the same chance of being picked, the answer to this is equivalent to the proportion of students that are taller than 70.5 inches. Using the eCDF we obtain an answer by typing:

```{r}
1 - F(70)
```
The CDF is a version of the eCDF that assigns theoretical probabilities for each $a$ rather than proportions computed from data. Although, as we just demonstrated, proportions computed from data can be used to define probabilities for a random variable.
Specifically, the CDF for a random outcome $X$ defines, for any number $a$, the probability of observing a value larger than $a$. 

$$ F(a) = \mbox{Pr}(X \leq a) $$

Once a CDF is defined, we can use it to compute the probability of any subset of values. For instance, the probability of a student being between height `a` and height `b` is:

$$
\mbox{Pr}(a < X \leq b) = F(b)-F(a)
$$

Because we can compute the probability for any possible event this way, the CDF defines the probability distribution.

## 13.10.2 Probability density function

A mathematical result that is actually very useful in practice is that for most CDFs we can define a function, call it $f(x)$, that permits us to construct the CDF using Calculus, like this:

$$
F(b) - F(a) = \int_a^b f(x)\,dx
$$
$f(x)$ is referred to as the _probability density function_. The intuition is that even for continuous outcomes we can define tiny intervals, that are almost as small as points, that have positive probabilities. If we think of the size of these intervals as the base of a rectangle,the probability density function $f$ determines the height of the rectangle so that the summing up the area of these rectangles approximate the probability  $F(b) - F(a)$. This sum can be written as Reimann sum that is approximated by an integral:

```{r}
cont <- 
  data.frame(x=seq(0,5, len = 300), 
             y=dgamma(seq(0,5, len = 300), 2, 2))
disc <- 
  data.frame(x=seq(0, 5, 0.075), 
             y=dgamma(seq(0, 5, 0.075), 2, 2))

ggplot(mapping = aes(x,y)) +
  geom_col(data =  disc) +
  geom_line(data = cont) +
  ylab("f(x)")
```

An example of such a continuous distribution is the normal distribution. As we saw in \@ref(normal-distribution), the probability density function is given by:


$$f(x) = e^{-\frac{1}{2}\left( \frac{x-m}{s} \right)^2} $$


The cumulative distribution for the normal distribution is defined by a mathematical formula which in R can be obtained with the function <span id="pnorm">__`pnorm`__</span>. We say that a random quantity is normally distributed with average `m` and standard deviation `s` if its probability distribution is defined by:

```
F(a) = pnorm(a, m, s)
```

This is useful because if we are willing to use the normal approximation for, say, height, we don't need the entire dataset to answer questions such as: what is the probability that a randomly selected student is taller then 70 inches? We just need the average height and standard deviation:

```{r}
m <- mean(x)
s <- sd(x)
1 - pnorm(70.5, m, s)
```

## 13.10.3 Theoretical distributions as approximations

The normal distribution is derived mathematically: we do not need data to define it. For practicing data scientists, almost everything we do involves data. Data is always, technically speaking, discrete. For example, we could consider our height data categorical with each specific height a unique category. The probability distribution is defined by the proportion of students reporting each height. Here is a plot of that probability distribution:

```{r}
rafalib::mypar()
plot(prop.table(table(x)), 
     xlab = "a = Height in inches", 
     ylab = "Pr(X = a)")
```

While most students rounded up their heights to the nearest inch, others reported values with more precision. One student reported his height to be 69.6850393700787, which is 177 centimeters. The probability assigned to this height is `r 1/length(x)` or 1 in `r length(x)`. The probability for 70 inches is much higher at `r mean(x==70)`, but does it really make sense to think of the probability of being exactly 70 inches as being different than 69.6850393700787? Clearly it is much more useful for data analytic purposes to treat this outcome as a continuous numeric variable, keeping in mind that very few people, or perhaps none, are exactly 70 inches, and that the reason we get more values at 70 is because people round to the nearest inch. 

<span class="blue">With continuous distributions, the probability of a singular value is not even defined.</span> For example, it does not make sense to ask what is the probability that a normally distributed value is 70. Instead, <span class="blue">we define probabilities for intervals.</span> We thus could ask what is the probability that someone is between 69.5 and 70.5.

In cases like height, in which the data is rounded, the normal approximation is particularly useful if we deal with intervals that include exactly one round number. For example, the normal distribution is useful for approximating the proportion of students reporting values in intervals like the following three:

```{r}
options(digits = 3)
mean(x <= 68.5) - mean(x <= 67.5)
mean(x <= 69.5) - mean(x <= 68.5)
mean(x <= 70.5) - mean(x <= 69.5)
```
Note how close we get with the normal approximation:
```{r}
pnorm(68.5, m, s) - pnorm(67.5, m, s)
pnorm(69.5, m, s) - pnorm(68.5, m, s)
pnorm(70.5, m, s) - pnorm(69.5, m, s)
```
However, <span class="blue">the approximation is not as useful for other intervals. For instance, notice how the approximation breaks down when we try to estimate: </span>
```{r}
mean(x <= 70.9) - mean(x <= 70.1)
pnorm(70.9, m, s) - pnorm(70.1, m, s)
```

In general, we call this situation __discretization__. Although the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding. As long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool.

## 13.10.4 The probability density

For categorical distributions, we can define the probability of a category. For example, a roll of a die, let's call it $X$, can be 1, 2, 3, 4, 5 or 6. The probability of 4 is defined as:

$$
\mbox{Pr}(X=4) = 1/6
$$

The CDF can then easily be defined:
$$
F(4) = \mbox{Pr}(X\leq 4) =  \mbox{Pr}(X = 4) +  \mbox{Pr}(X = 3) +  \mbox{Pr}(X = 2) +  \mbox{Pr}(X = 1) 
$$

Although for continuous distributions the probability of a single value $\mbox{Pr}(X=x)$ is not defined, there is a theoretical definition that has a similar interpretation. The probability density at $x$ is defined as the function $f(a)$ such that:

$$
F(a) = \mbox{Pr}(X\leq a) = \int_{-\infty}^a f(x)\, dx
$$

For those that know calculus, remember that the integral is related to a sum: it is the sum of bars with widths approximating 0. If you don't know calculus, you can think of $f(x)$ as a curve for which the area under that curve up to the value $a$, gives you the probability $\mbox{Pr}(X\leq a)$. 

For example, to use the normal approximation to estimate the probability of someone being taller than 76 inches, we use:

```{r}
1 - pnorm(76, m, s)
```

which mathematically is the grey area below:

```{r}
dat <- tibble(x = seq(-4, 4, length=100) * s + m,
              y = dnorm(x, m, s))
dat_ribbon <- filter(dat, x >= 2 * s + m)
ggplot(dat, aes(x, y)) +
  geom_line() +
  geom_ribbon(aes(ymin = 0, ymax = y), data = dat_ribbon)
```

The curve you see is the probability density for the normal distribution. In R, we get this using the function `dnorm`.

Although it may not be immediately obvious why knowing about probability densities is useful, understanding this concept will be essential to those wanting to fit models to data for which predefined functions are not available.

# 13.11 Monte Carlo simulations for continuous variables

R provides functions to generate normally distributed outcomes. Specifically, the <span id="rnorm">__`rnorm`__</span> function takes three arguments: size, average (defaults to 0), and standard deviation (defaults to 1) and produces random numbers. Here is an example of how we could generate data that looks like our reported heights:

```{r}
n <- length(x)
m <- mean(x)
s <- sd(x)
simulated_heights <- rnorm(n, m, s)
```

Not surprisingly, the distribution looks normal:

```{r}
data.frame(simulated_heights = simulated_heights) %>% 
  ggplot(aes(simulated_heights)) +
  geom_histogram(color = "black", binwidth = 1)
```

This is one of the most useful functions in R as it will permit us to generate data that mimics natural events and answers questions related to what could happen by chance by running Monte Carlo simulations.

If, for example, we pick 800 males at random, what is the distribution of the tallest person? How rare is a seven footer in a group of 800 males? The following Monte Carlo simulation helps us answer that question:

```{r}
B <- 10000
tallest <- replicate(B, {
  simulated_data <- rnorm(800, m, s)
  max(simulated_data)
})
```
Having a seven footer is quite rare:
```{r}
mean(tallest >= 7*12)
```
```{r}
data.frame(tallest = tallest) %>% 
  ggplot(aes(tallest)) +
  geom_histogram(color = "black", binwidth = 1)
```

<span class="blue">Note that it does not look normal.</span>

# 13.12 Continuous distributions

The normal distribution is not the only useful theoretical distribution.  Other continuous distributions that we may encounter are the student-t, Chi-square, exponential, gamma, beta, and beta-binomial. R provides functions to compute the density, the quantiles, the cumulative distribution functions and to generate Monte Carlo simulations. R uses a convention that lets us remember the names, namely using the letters `d`, `q`, `p`, and `r` in front of a shorthand for the distribution. We have already seen the functions `dnorm`, `pnorm`, and `rnorm` for the normal distribution. The functions <span id="qnorm">__`qnorm`__</span> gives us the quantiles. We can therefore draw a distribution like this:

```{r}
x <- seq(-4, 4, length.out = 100)
# qplot(x, f, geom = "line", data = data.frame(x, f = dnorm(x)))
data.frame(x = x, f = dnorm(x)) %>% 
  ggplot(aes(x, f)) +
  geom_line()
```

For the student-t, described later in Section \@ref(t-dist), the shorthand `t` is used so the functions are `dt` for the density, `qt` for the quantiles, `pt` for the cumulative distribution function, and `rt` for Monte Carlo simulation.



















